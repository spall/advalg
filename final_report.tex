\documentclass[letterpaper,11pt,titlepage]{article}
\usepackage{epsfig,xspace,url}
\usepackage{authblk}
\usepackage[margin=.5in]{geometry}

\iffalse 
\addtolength{\oddsidemargin}{-.25in}
\addtolength{\evensidemargin}{-.25in}
\addtolength{\textwidth}{.5in}

\addtolength{\topmargin}{-.25in}
\addtolength{\textheight}{1in}

\fi

\title{Linear Approximation of Shortest Common Superstrings}
\author[1]{Sarah Spall and Scott Bauer}

\begin{document}
\maketitle
\newpage

\subsection*{Introduction}
A superstring of a set of strings $S$, is a string $w$, such that each string $s \in S$ is a substring
of w.  The goal of shortest common superstring is to find the shortest possible $w$, such that each 
string $s \in S$ is a substring of $w$. This problem has found application in both DNA sequencing 
and data compression.  DNA is made up of four neucleotides, A, C, G, and T.  Sequencing molecules,
determining a ``string'' over the alphabet of neucleotides, is/was an important part of biology.  To 
create the sequence of a molecule, fragments (pieces of the molecule), are used to find the shortest
common superstring of the fragments; this seems to work well in practice.  \cite{li1990towards}

 
This problem has been shown to be NP-hard/NP-complete \cite{blum1991linear} \cite{johngallant1980}, 
so research has focused on finding approximation algorithms for the problem.  The first approximation 
algorithm seems to be a greedy algorithm discussed in Gallant (1982). This is now a commonly used greedy 
algorithm, addressed in newer papers.  If we have a set $S$, the algorithm works by finding two strings $s$
and $t$ in $S$, that have maximum overlap.  The overlap string $o$ is formed from $s$ and $t$, and $o$ is
placed back in $S$.  Strings $s$ and $t$ are then removed from $S$.  This process is repeated until
$S$ contains one string, or there is no non-empty overlap between strings in $S$.  This algorithm is later
addressed in \cite{turner1989approximation}, \cite{tarhio1988greedy}, \cite{li1990towards}, and \cite{blum1991linear}.
\cite{tarhio1988greedy} developed an algorithm that implemented the greedy approximation, and conjectured that 
the length of the superstring produced by their algorithm is less than or equal to $2*len(OPT)$.  They were unable
to prove this however, and it remained an open problem.  \cite{turner1989approximation} was also unable to prove
the conjectured $2*len(OPT)$ upper bound of the greedy approximation.  


\subsection{Related Work}
Work on the Shortest common superstring problem has been around since 1977. In 1980 Gallant et al. in \textit{On Finding Minimal length superstrings}\cite{gallant1980finding} set the basis for work on the shortest common superstring problem. 

In Gallant, the researchers prove that given a set of strings $S$ and a positive length $K$ as well as an unbounded alphabet $\sum$ over those strings, that determining if $S$ has a superstring of length $K$ is NP-Complete. The authors do this by reducing to the \textit{Hamiltonian  path problem}. The Hamiltonian path problem is the problem where one determines in a directed or undirected graph whether there exists a path that visits each vertex only once. The Hamiltonian path problem was known to be NP-Complete so a reduction ultimately proves the completeness of the other problem \cite{michael1979computers}. Throughout all newer papers on the SCS problem each paper reduces the problem to the Hamiltonian path problem while using different tricks to reduce the SCS.\\


In 1988 Jorma Tarhio and Esko Ukkonen, authored \textit{A Greedy Approximation Algorithm for Constructing Common Superstrings} \cite{tarhio1988greedy}. Their paper developed a polynomial-time approximation algorithm that would construct a superstring within some $\epsilon$ of the optimal solution. Their approxmiation algorithm is based off a greedy heuristic. At a high level their algorithm is given a set $S$ of strings over the alphabet $\sum$ select two strings which have the greatest mutual overlap. From there the algorithm merges the two strings and replaces the single combined string in $S$. An example would be the set $S = {aabb, bbbaaa, aaaa}$ The algorithm would select $bbbaaa$ and $aaaa$, merging and replacing so the set looks like $S = { aabb, bbbaaaa }$. The algorithm at finer detail is implemented using, once again, Hamiltonian paths.The algorithm constructs a longest Hamiltonian path for weighted directed graph. The weight of the edges are the overlap between strings. The greedy heuristic holds on this graph as it chooses the edges with the maximum weight. Although their algorithm constructs a solution they do not prove the lower bound of the algorithm and leave it open for future work. \\

Aside from compression of strings in computation, the SCS problem has relevance in the biology field. In \textit{Towards a DNA Sequencing Theory}, Ming Li explains the relevance of SCS in terms of DNA sequencing (in 1990). The idea of sequencing DNA is an important one to understand the basis of life. Cures for diseases, viruses, among other things all come from information that is derived from sequencing DNA. At a high level DNA strands can be encoded from a set of nucleotides $A, T, C, G$. This is important information. Using this reduction scientists can reduce DNA to a string which can then be used for the SCS problem. Using the SCS problem on this DNA string was very relevant in the early 90's due to the limited technology of sequencind DNA. Only around 500 base pairs could be sequenced at a time. If A scientist can get the most out of those 500 pairs then their research can continue. The paper provides an algorithm in which they argue it constructs a superstring of length $O(nlogn)$ where n is the smallest possible substring, ie the optimal soltuion. The algorithm provided in the paper takes the base greedy approximation algorithm, but instead of greedily choosing two strings which have the greatest overlap, the algorithm chooses two strings such that their merge will make it so every other group of strings has some overlap. That is he tries to maximize the number of strings in which the merge will overlap with. They call this technique group merging.\\

\subsection{Related Work/Prior work}

Prior to Blum et. al. work on the shortest common superstring problem was focused on finding approximation algorithms because the shortest common superstring was shown to be NP-hard by Gallant et al. 1980.  
TALK ABOUT GALLANT

Tarhio and Ukkonen (1988) developed a ``greedy'' approximation algorithm based on an idea first mentioned in Gallant 1982; Find the two strings in a set of strings, with the largest overlap, and merge these two strings together to form a new overlapped string.  Then add new overlapped string to the set of strings, and remove two original strings.  Continue to do this until there is only a single string in the set.  The main result of this paper dealt with the quality of the approximation, when using ``compression'' as the measure of quality.  They show that if $n$ is the sum of the length of every string in set $S$, and $k_{min}$ is length of the shortest common superstring for $S$, and $k_{greedy}$ is the length of the string produced by their greedy approximation algorithm, then $(n - k_{greedy}) \geq \frac{1}{2}*(n - k_{min})$.  There is another way to compare the quality of an approximation algorithm for the shortest common superstring problem, compare the length of the common superstring created by the approximation for a set of strings $S$ to the length of the shortest common superstring for the set of strings $S$.  Tarhio and Ukkonen were unable to proven an upper bound for this quality measure, but conjectured that their approximation algorithm produced common superstrings  of length no more than $2$ times the length of the shortest common superstring; $k_{greedy} \leq 2*k_{min}$.  \\
\\
All work was considered on a reduced set of strings, meaning no string in a set $S$ is a substring of another string in $S$, because a reduced set of strings will have the same substring as the non-reduced set.\\
The shortest common superstring (SCS) problem can be viewed as a special case of the longest hamiltonian path problem on a weighted directed graph, and an approximation solution for SCS was approached as finding the longest hamiltonian path in a weighted directed graph by more than one paper \cite{tarhio1988greedy} \cite{turner1989approximation}.  An overlap between two strings $s_i$ and $s_j$, where $s_i \neq s_j$, $s_i = u*v$ and $s_j = v*w$, is $v$; $v$ can be the empty string.  Let $ov(s_i, s_j)$, be the length of the maximal overlap between $s_i$ and $s_j$.  The definition of overlap is assymetric, an overlap between $s_i$ and $s_j$ does not mean there is an overlap between $s_j$ and $s_i$.  An overlap is maximal if it is as large as possible.  \\ \\

Tarhio defines an ``overlap'' graph $G$ for a set of strings $S = {s_1, ... ,s_n}$, with vertex set $S \cup {x_o, x_{n+1}}$, and with directed edges $(s_i, S_j)$, if $s_i \neq s_j$.  $x_o$ is a start node, and $x_{n+1}$ is an end node.  Each edge $(s_i, s_j)$ has weight $ov(s_i, s_j)$.  The weight of the edge from $x_o$ to each other vertex is $0$, and the weight from $x_{n+1}$ to every other vertex is $0$.  So, the ``overlap'' graph, is a graph $G = (V, E, ov)$.  From any directed hamiltonian path $H$ on $G$ from $x_o$ to $x_{n+1}$, a common superstring can be constructed for $S$.  The SCS for $s$ would be constructed then from the longest hamiltonina path on $G$.  
The paper implies that finding the longest Hamiltonian path is NP-hard, but this is already well known.  Since finding the longest Hamiltonian path is NP-hard an approximation algorithm is needed, and there is a well known ``greedy'' heuristic for longest Hamiltonian paths.  To construct a Hamiltonian path, select an edge $e$ from the remaining edges in $G$, the overlap graph, such that $e$ has the largest weight and $e$ plus the edges already selected form a subgraph that can be expanded to a Hamiltonian path.  This is repeated until a Hamiltonian path has been found.  Tarhio used this ``greedy'' approximation algorithm as part of their approximation algorithm for SCS.  \\ \\

Tarhio's proposed algorithm finds the maximal overlap between every string in a set of strings $S$, and they use the Knuth-Morris-Pratt algorithm to calculate maximum overlaps.  During this step of the algorithm, if a string is found to be a substring of another string, it is removed from the set $S$.  From this reduced set of strings $S$ and using the calculated maximum overlaps between strings, the previously described overlap graph $G$ can be constructed and the ``greedy'' approximation algorithm can be run on $G$.  \\ \\

Tarhio approached the problem of an approximation algorithm for SCS by reducing the problem to the longest Hamiltonian path problem and using a well known greedy approximation algorithm for longest hamiltonian path, to generate a common superstring.  They also gave a guaranteed bound with regards to ``compression''.  And conjectured  that their approximation algorithm produces strings of length less than or equal to $2$ times the length of the SCS.   \\ \\

Another slightly later paper Turner (1989) \cite{turner1989approximation} approached an approximation algorithm as Tarhio did, thinking about it from the viewpoint of maximizing the overlap between the strings in a set $S$.  Just as Tarhio, Turner could find no worse example than $k_{greedy} \leq 2* k_{min}$, but was also not able to prove the upper bound.  Turner also showed $(n - k_{greedy}) \geq \frac{1}{2}*(n - k_{min})$.  \\ \\ 

Turner models the problem of finding the SCS as the longest path problem, on a complete directed graph, with each edge have a non-negative length, where the weight is the overlap between a string $s_i$ and $s_j$.  In this specific case of the Longest Path Problem, the goal is to find a Hamiltonian path that maximized the sum of the weight of the edges of the path.  At this point Turner has described almost the exact same overlap graph described in Tarhio.  \\ \\
Turner then presents 3 different approximation algorithms for SCS. The first approximation algorithm finds a maximum matching in the overlap graph $G$.  Since $G$ is complete, any matching can be extended to a path, and the maximum matching is used to find a Hamiltonian path.  This is similar to the approach of Tarhio, except instead of using the ``greedy'' approximation algorithm for longest Hamiltonian paths, he used maximum matchings, to construct Hamiltonian paths.  Turner's second approximation algorithm found a directed matching in the overlap graph $G$. The directed matching was also used to find a Hamiltonian path.  For his third approximation algorithm Turner mentions a ``greedy'' approximation for the longest path problem.  This algorithm is the same greedy algorithm considered in Tarhio.  \\ \\

Turner then relates SCS to the path version of the traveling salesman problem; This is similar to what is done in Blum et al. ()  This is an asymmetric case of the traveling salesman problem where the triangle inequality holds, meaning it is not NP-hard to find an approximation algorithm.  

\subsection{Main results}


\newpage

{
  %\footnotesize 
  \small 
  \bibliographystyle{acm}
  \bibliography{biblio}
}

\end{document}
