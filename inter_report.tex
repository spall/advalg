\documentclass[letterpaper,twocolumn,12pt]{article}
\usepackage{epsfig,xspace,url}
\usepackage{authblk}


\title{Linear Approximation of Shortest Superstrings report}
\author[1]{Sarah Spall And Scott Bauer}


\begin{document}

\maketitle

\subsection*{Problem Being Solved}

previously defined shortest super strings problem
the problem is known to be NP-hard, but there is a 
greedy solution that at the time of the paper was used
in DNA sequencing and data compression.  

work done in this paper:
show ``greedy algo'' has upper bound of 4n, when only
previous nontrivial algo is O(nlogn)???



\subsection*{Main Result}

present new simplified greedy algo that produces
a superstring of length at most 3n.  

so they improve on previous work, by setting new upper bound
and new greedy algo (upper bound of the length)

also show problem to be MAX SNP-hard, which implies 
ptas is unlikely? what is ptas



\subsection*{Why result is important}

This result is, in our opinion, most important to the field of biology. To understand why this improvement is important we must quickly rewview some biology 101. Chromosomes, which everyone has, contains our DNA, which in turn carries our genes, among other life important information. In bioinformatics scientists need to sequence DNA. DNA sequencing is the process of determining the order of neucleotides in DNA. Sequencing DNA is an extremely important portion of biology, which helps us determing if a child is ours, determine how a virus evolves and the list continues on. 

The issue with DNA sequencing, at least in the early 90's, was technology was not very good. In early 90's, the methods of sequencing only allowed for a few hundred neucelotide fragments to be generated \cite{karp1993mapping}. Because of this technologicial limitation, scientist would look for the best sequence of the entire DNA molecule. The best sequence is the shortest sequence such that it contains each fragment of the DNA. As you can see, the DNA problem can be mapped onto a shortest supersequence problem. 

Because the authors of this paper were able to improve upon the upper bound length of the approximated best superstring from $4n$ to $3n$ it allowed bioinformatic researchers to sequence DNA more accurately. 



\subsection*{Explain the impact}
Explain what impact (if any) this result has had (or might have):

\subsection*{How the paper does what it does}

previous papers defined a greedy approximation algorithm for the shortest common superstring
problem, and conjectured that the greedy algorithm would return a superstring no longer than 
2*n, where n is the length of the shortest common superstring.  [10][11]
\\
[4] provides the first provably good approximation algorithm for the problem, which produces
a superstring of at most n log n length, where n is the optimal lenght.
[4] proposes a new algorithm, group-merge that starts with
an input set S and an empty set T

How does what [4] proposes differ from algos in [10][11]?

What bottleneck did these other papers hit?
none?

Bottleneck?  all conjecture that algo is actually 2n but can't prove it, so this paper
takes steps towards proving the constant*n bound.

This paper proposes concat cycles, which produces string of length at most 4*n
then present mcgreedy which mimics concat-cycles

mgreedy has a Set of strings S and an empty set T, just like [4]
then while S is not empty choose s and t in S, where overlap is maximized.  if s != t
then remove them from S, and replace with merge(s,t).  if s = t, remove s from S and add it to T

when S is empty output concat of strings in T


{
  %\footnotesize 
  \small 
  \bibliographystyle{acm}
  \bibliography{biblio}
}

\end{document}

