\documentclass[letterpaper,twocolumn,11pt]{article}
\usepackage{epsfig,xspace,url}
\usepackage{authblk}
\addtolength{\oddsidemargin}{-.25in}
\addtolength{\evensidemargin}{-.25in}
\addtolength{\textwidth}{.5in}

\addtolength{\topmargin}{-.25in}
\addtolength{\textheight}{1in}

\title{Linear Approximation of Shortest Superstrings report}
\author[1]{Sarah Spall And Scott Bauer}


\begin{document}

\maketitle
\newpage

\subsection*{Problem Being Solved}

This paper improves upon an algorithm GREEDY \cite{tarhio1988greedy},which is the \textit{shortest common superstring problem}. The GREEDY algorithm is an approximation algorithm which given a finite set of strings $F$ where each string comes from some finite or infinite alphabet $\sum$ it can find an approximate shortest string $w$ such that each string $s \in F$ is a substring of $w$. The paper proves upper bounds on the GREEDY algorithm, as well as improves upon the GREEDY algorithm to produce a better upper bound. The GREEDY algorithm, and the improved upon algorithm are approximation algorithms, because the problem is proven to be NP-Hard. \cite{gallant1980finding}
 


\subsection*{Main Result}

The Main result of this paper is a proof that the GREEDY algorithm has an upper bound approximated shortest superstring of at most length $4n$. They then create an algorithm TGREEDY, which they prove has a upper bound length of $3n$. Lastly, the authors show that the problem itself is MAX SNP-hard.

\subsection*{Why result is important}

This result is, in our opinion, most important to the field of biology. To understand why this improvement is important we must quickly rewview some biology 101. Chromosomes, which everyone has, contains our DNA, which in turn carries our genes, among other life important information. In bioinformatics scientists need to sequence DNA. DNA sequencing is the process of determining the order of neucleotides in DNA. Sequencing DNA is an extremely important portion of biology, which helps us determing if a child is ours, determine how a virus evolves and the list continues on. 

The issue with DNA sequencing, at least in the early 90's, was technology was not very good. In early 90's, the methods of sequencing only allowed for a few hundred neucelotide fragments to be generated \cite{karp1993mapping}. Because of this technologicial limitation, scientist would look for the best sequence of the entire DNA molecule. The best sequence is the shortest sequence such that it contains each fragment of the DNA. As you can see, the DNA problem can be mapped onto a shortest supersequence problem. 

Because the authors of this paper were able to improve upon the upper bound length of the approximated best superstring from $4n$ to $3n$ it allowed bioinformatic researchers to sequence DNA more accurately. 



\subsection*{Explain the impact}
As explained in the previous section the most significant impact came for the biology field. But in terms of computer science there isn't any huge impact. The only noticable impact would come in compression \cite{storer1988data}. Obviously, if you care to encode a string as a minimum superstring, each improvement on the approximated length of the superstring will get you better compression. Lastly, the paper showed that the GREEDY algorithm could be improved upon. Since the publication we were able to find 8 papers which improved the upper bound from $3n$ to, $2 \frac{8}{9}n$, $2 \frac{5}{6}n$, $2 \frac{50}{63}n$, $2 \frac{3}{4}n$, $2 \frac{2}{3}n$, $2.596n$ and finally $2 \frac{1}{2}n$ in 2000 \cite{sweedyk2000boldmath}.



\subsection*{How the paper does what it does}
Previous papers \cite{tarhio1988greedy} \cite{turner1989approximation} presented a greedy approximation algorithm that was conjectured to produce a superstring of length 2n, where n is the length of the optimal shortest common superstring.  The greedy approximation algorithm finds a pair of strings s and t in R, which have the longest mutual overlap amongst all possible pairs in R, and then removes s and t from R.  The overlapped string of s and t is then placed in R.  This is repeated until R contains one string, or no two strings have a non-empty overlap. \cite{tarhio1988greedy} \\
Li \cite{li1990towards} proposes a different algorithm that provides the first provably good approximation algorithm for the shortest common superstring problem, with a bound of O(n logn).  The idea behind the algorithm is to merge large groups.  Each time two strings are mreged the algorithm tries to choose to merge two strings in a way that would make other strings in the set become substrings.  \\
\\
Previous work failed to prove that an algorithm has the O(n) conjectured bound.  Blum, Avrim, et al. \cite{blum1991linear} present a new modified greedy approximation algorithm, TGREEDY, and prove that it produces superstrings of length at most 3n, where n is the length of the optimal superstring.  \\


\cite{blum1991linear} relates shortest common superstring problem to the traveling salesman problem, as \cite{turner1989approximation} did.  Then \cite{blum1991linear} goes on to describe an algorithm concat-cycles which decomposes a weighted directed graph into cycles such that each vertex is in one cycle and the total weight of the cycles is minimized.  It is then proved that this algorithm produces a string of length at most 4*n.  MGREEDY, an algorithm that mimics the previously presented concat-cycles algorithm, is then presented.

How does what \cite{li1990towards} proposes differ from algos in \cite{tarhio1988greedy}\cite{turner1989approximation}?


This paper proposes concat cycles, which produces string of length at most 4*n
then present mcgreedy which mimics concat-cycles

mgreedy has a Set of strings S and an empty set T, just like \cite{li1990towards}
then while S is not empty choose s and t in S, where overlap is maximized.  if s != t
then remove them from S, and replace with merge(s,t).  if s = t, remove s from S and add it to T

when S is empty output concat of strings in T


{
  %\footnotesize 
  \small 
  \bibliographystyle{acm}
  \bibliography{biblio}
}

\end{document}

